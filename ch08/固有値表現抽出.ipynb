{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "固有値表現抽出.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVgOInp9vQ6vkWUq3Hvy6M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWepx7LIdpSy",
        "outputId": "76f54047-9dea-4af0-df02-7d1e416010d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.5.0 in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: fugashi==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: ipadic==1.0.0 in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pytorch-lightning==1.2.7 in /usr/local/lib/python3.7/dist-packages (1.2.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (0.0.49)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7) (1.10.0+cu111)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7) (0.18.2)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7) (2022.3.0)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7) (6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7) (0.7.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7) (2.8.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (3.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7) (57.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7) (3.2.0)\n",
            "Requirement already satisfied: pyDeprecate==0.3.* in /usr/local/lib/python3.7/dist-packages (from torchmetrics>=0.2.0->pytorch-lightning==1.2.7) (0.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (1.7.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7) (2.0.12)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (3.0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import itertools\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
        "\n",
        "# 日本語学習済みモデル\n",
        "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "id": "TzDXkmGhd2yd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eni5VopqdcN9",
        "outputId": "43972bd9-1afb-4834-942a-e92334f630c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chap8/chap8/chap8\n"
          ]
        }
      ],
      "source": [
        "!mkdir chap8\n",
        "%cd ./chap8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 固有表現のデータ表現"
      ],
      "metadata": {
        "id": "IfqkV83fer14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'AさんはBCD株式会社を起業した。'\n",
        "entities = [\n",
        "    {'name':'A', 'span':[0,1], 'type':'人名', 'type_id':1},\n",
        "    {'name':'BCD株式会社', 'span':[4,11], 'type':'組織名', 'type_id':2},\n",
        "]"
      ],
      "metadata": {
        "id": "QaiC0bVqe_qZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 固有表現抽出の実装：IO法"
      ],
      "metadata": {
        "id": "mgp7s5N-f5KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NER_tokenizer(BertJapaneseTokenizer):\n",
        "       \n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "        \"\"\"\n",
        "        文章とそれに含まれる固有表現が与えられた時に、\n",
        "        符号化とラベル列の作成を行う。\n",
        "        \"\"\"\n",
        "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "        entities = sorted(entities, key=lambda x: x['span'][0])\n",
        "        splitted = [] # 分割後の文字列を追加していく\n",
        "        position = 0\n",
        "        for entity in entities:\n",
        "            start = entity['span'][0]\n",
        "            end = entity['span'][1]\n",
        "            label = entity['type_id']\n",
        "            # 固有表現ではないものには0のラベルを付与\n",
        "            splitted.append({'text':text[position:start], 'label':0}) \n",
        "            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n",
        "            splitted.append({'text':text[start:end], 'label':label}) \n",
        "            position = end\n",
        "        splitted.append({'text': text[position:], 'label':0})\n",
        "        splitted = [ s for s in splitted if s['text'] ] # 長さ0の文字列は除く\n",
        "\n",
        "        # 分割されたそれぞれの文字列をトークン化し、ラベルをつける。\n",
        "        tokens = [] # トークンを追加していく\n",
        "        labels = [] # トークンのラベルを追加していく\n",
        "        for text_splitted in splitted:\n",
        "            text = text_splitted['text']\n",
        "            label = text_splitted['label']\n",
        "            tokens_splitted = self.tokenize(text)\n",
        "            labels_splitted = [label] * len(tokens_splitted)\n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids, \n",
        "            max_length=max_length, \n",
        "            padding='max_length', \n",
        "            truncation=True\n",
        "        ) # input_idsをencodingに変換\n",
        "        # 特殊トークン[CLS]、[SEP]のラベルを0にする。\n",
        "        labels = [0] + labels[:max_length-2] + [0] \n",
        "        # 特殊トークン[PAD]のラベルを0にする。\n",
        "        labels = labels + [0]*( max_length - len(labels) ) \n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "    def encode_plus_untagged(\n",
        "        self, text, max_length=None, return_tensors=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークンを追加していく。\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens) \n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids, \n",
        "            max_length=max_length, \n",
        "            padding='max_length' if max_length else False, \n",
        "            truncation=True if max_length else False\n",
        "        )\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
        "\n",
        "        # 必要に応じてtorch.Tensorにする。\n",
        "        if return_tensors == 'pt':\n",
        "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, labels, spans):\n",
        "        \"\"\"\n",
        "        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n",
        "        \"\"\"\n",
        "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
        "        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n",
        "        spans = [span for span in spans if span[0] != -1]\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        for label, group \\\n",
        "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "            \n",
        "            group = list(group)\n",
        "            start = spans[group[0][0]][0]\n",
        "            end = spans[group[-1][0]][1]\n",
        "\n",
        "            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n",
        "                entity = {\n",
        "                    \"name\": text[start:end],\n",
        "                    \"span\": [start, end],\n",
        "                    \"type_id\": label\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities"
      ],
      "metadata": {
        "id": "QVxNvfk1f_kZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "CAd-M0A4n1J7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '昨日のみらい事務所との打ち合わせは順調だった。'\n",
        "entities = [\n",
        "    {'name':'みらい事務所', 'span':[3,9], 'type_id':1}\n",
        "]\n",
        "\n",
        "encoding = tokenizer.encode_plus_tagged(\n",
        "    text,\n",
        "    entities,\n",
        "    max_length=20\n",
        ")\n",
        "print(encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lwDmJb5n6I9",
        "outputId": "18c97abe-6993-4656-c62a-b6c2ebab22e9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [2, 10271, 28486, 5, 546, 10780, 2464, 13, 5, 1878, 2682, 9, 10750, 308, 10, 8, 3, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'labels': [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '騰訊の英語名はTencent Holdings Ltdである。'\n",
        "encoding, spans = tokenizer.encode_plus_untagged(\n",
        "    text, return_tensors='pt'\n",
        ")\n",
        "print('# encoding')\n",
        "print(encoding)\n",
        "print('# spans')\n",
        "print(spans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDhBSvE6oqCG",
        "outputId": "31ada852-2dce-47a0-d8b4-b77953123028"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# encoding\n",
            "{'input_ids': tensor([[    2,     1, 26280,     5,  1543,   125,     9,  6749, 28550,  2953,\n",
            "         28550, 28566, 21202, 28683, 14050, 12475,    12,    31,     8,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "# spans\n",
            "[[-1, -1], [0, 1], [1, 2], [2, 3], [3, 5], [5, 6], [6, 7], [7, 9], [9, 10], [10, 12], [12, 13], [13, 14], [15, 18], [18, 19], [19, 23], [24, 27], [27, 28], [28, 30], [30, 31], [-1, -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_predictes = [0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0]\n",
        "entities = tokenizer.convert_bert_output_to_entities(\n",
        "    text,\n",
        "    labels_predictes,\n",
        "    spans\n",
        ")\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPijazuXpoej",
        "outputId": "5e0681ca-b916-479a-a9ce-c91fcf946c12"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': '騰訊', 'span': [0, 2], 'type_id': 1}, {'name': 'Tencent Holdings Ltd', 'span': [7, 27], 'type_id': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTによる固有表現抽出"
      ],
      "metadata": {
        "id": "Gby7qY9PrTrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
        "bert_tc = BertForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=4\n",
        ")\n",
        "bert_tc = bert_tc.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHdXYyVgqM_3",
        "outputId": "214a5b5f-97cd-4761-ad35-f7b9f1a46784"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import log\n",
        "text = 'AさんはB大学に入学した。'\n",
        "\n",
        "# 符号化を行い、各トークンの文章中での位置も特定しておく\n",
        "encoding, spans = tokenizer.encode_plus_untagged(\n",
        "    text,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "encoding = {k:v.cuda() for k, v in encoding.items()}\n",
        "\n",
        "# BERTでトークン毎の分類スコアを出力し、スコアの最も高いラベルを予測値をする\n",
        "with torch.no_grad():\n",
        "    output = bert_tc(**encoding)\n",
        "    scores = output.logits\n",
        "    labels_predictes = scores[0].argmax(-1).cpu().numpy().tolist()\n",
        "\n",
        "# ラベル列を固有表現に変換\n",
        "entities = tokenizer.convert_bert_output_to_entities(\n",
        "    text,\n",
        "    labels_predictes,\n",
        "    spans\n",
        ")\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfctzP7Pq79X",
        "outputId": "fa03ef95-0f79-43ee-8c5d-21e3461faf3f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'Aさんは', 'span': [0, 4], 'type_id': 1}, {'name': 'B', 'span': [4, 5], 'type_id': 3}, {'name': '大学に入学', 'span': [5, 10], 'type_id': 1}, {'name': 'た。', 'span': [11, 13], 'type_id': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils.dummy_pt_objects import AdaptiveEmbedding\n",
        "data = [\n",
        "    {\n",
        "        'text': 'AさんはB大学に入学した。',\n",
        "        'entities':[\n",
        "            {'name':'A', 'span':[0,1], 'type_id':2},\n",
        "            {'name':'B大学', 'span':[4,7], 'type_id':1}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        'text': 'CDE株式会社は新商品「E」を販売する。',\n",
        "        'entities':[\n",
        "            {'name':'CDE株式会社', 'span':[0,7], 'type_id':1},\n",
        "            {'name':'E', 'span':[12,13], 'type_id':3}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# 各データを符号化し、データローダを作成する\n",
        "max_length=32\n",
        "dataset_for_loader = []\n",
        "for sample in data:\n",
        "    text = sample['text']\n",
        "    entities = sample['entities']\n",
        "    encoding = tokenizer.encode_plus_tagged(\n",
        "        text,\n",
        "        entities,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    encoding = {k:torch.tensor(v) for k, v in encoding.items()}\n",
        "    dataset_for_loader.append(encoding)\n",
        "dataloader = DataLoader(dataset_for_loader, batch_size=len(data))\n",
        "\n",
        "# ミニバッチを取り出し損失を得る\n",
        "for batch in dataloader:\n",
        "    batch = {k:v.cuda() for k, v in batch.items()}\n",
        "    output = bert_tc(**batch)\n",
        "    loss = output.loss"
      ],
      "metadata": {
        "id": "l31Phm7lsfmc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセット：Wikipediaを用いら日本語の固有表現抽出データセット"
      ],
      "metadata": {
        "id": "4Phe7WnHuYRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ropn-F4dvgvX",
        "outputId": "da109bdf-7207-4866-f9d9-e6bd167b175d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ner-wikipedia-dataset'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 32 (delta 9), reused 11 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n",
            "Note: checking out 'f7ed83626d90e5a79f1af99775e4b8c6cba15295'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "dataset = json.load(open('ner-wikipedia-dataset/ner.json', 'r'))\n",
        "\n",
        "# 固有表現のタイプとIDを対応付ける辞書\n",
        "type_id_dict = {\n",
        "    '人名':1,\n",
        "    '法人名':2,\n",
        "    '政治的組織名':3,\n",
        "    'その他の組織名':4,\n",
        "    '地名':5,\n",
        "    '施設名':6,\n",
        "    '製品名':7,\n",
        "    'イベント名':8,\n",
        "}\n",
        "\n",
        "# カテゴリーをラベルに変更、文字列の正規化する\n",
        "for sample in dataset:\n",
        "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
        "    for e in sample['entities']:\n",
        "        e['type_id'] = type_id_dict[e['type']]\n",
        "        del e['type']\n",
        "\n",
        "# データセットの分割\n",
        "random.shuffle(dataset)\n",
        "n = len(dataset)\n",
        "n_train = int(n*0.6)\n",
        "n_val = int(n*0.2)\n",
        "dataset_train = dataset[:n_train]\n",
        "dataset_val = dataset[n_train:n_train+n_val]\n",
        "dataset_test = dataset[n_train+n_val:]"
      ],
      "metadata": {
        "id": "q5LB1jXwvwXd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ファインチューニング"
      ],
      "metadata": {
        "id": "x_pjBYKLx-Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(tokenizer, dataset, max_length):\n",
        "    \"\"\"\n",
        "    データセットをデータローダに入力できる形に整形\n",
        "    \"\"\"\n",
        "    dataset_for_loader = []\n",
        "    for sample in dataset:\n",
        "        text = sample['text']\n",
        "        entities = sample['entities']\n",
        "        encoding = tokenizer.encode_plus_tagged(\n",
        "            text,\n",
        "            entities,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        encoding = {k:torch.tensor(v) for k, v in encoding.items()}\n",
        "        dataset_for_loader.append(encoding)\n",
        "    return dataset_for_loader\n",
        "\n",
        "# トークナイザのロード\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# データセットの作成\n",
        "max_length = 128\n",
        "dataset_train_for_loader = create_dataset(\n",
        "    tokenizer,\n",
        "    dataset_val,\n",
        "    max_length\n",
        ")\n",
        "dataset_val_for_loader = create_dataset(\n",
        "    tokenizer,\n",
        "    dataset_val,\n",
        "    max_length\n",
        ")\n",
        "\n",
        "# データローダの作成\n",
        "dataloader_train = DataLoader(\n",
        "    dataset_train_for_loader,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
      ],
      "metadata": {
        "id": "HKsThohbyBKK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Lightningのモデル\n",
        "class BertForTokenClassification_pl(pl.LightningModule):\n",
        "    def __init__(self, model_name, num_labels, lr):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.bert_tc = BertForTokenClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        output = self.bert_tc(**batch)\n",
        "        loss = output.loss\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        output = self.bert_tc(**batch)\n",
        "        val_loss = output.loss\n",
        "        self.log('val_loss', val_loss)\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "checkpoint = pl.callbacks.ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top=1,\n",
        "    save_weights_only=True,\n",
        "    dirpath='model/'\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    max_epochs=5,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# ファインチューニング　\n",
        "model = BertForTokenClassification_pl(\n",
        "    MODEL_NAME,\n",
        "    num_labels=9,\n",
        "    lr=1e-5\n",
        ")\n",
        "trainer.fit(model, dataloader_train, dataloader_val)\n",
        "best_model_path = checkpoint.best_model_path"
      ],
      "metadata": {
        "id": "BifwXZknzgaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 固有表現抽出の性能評価"
      ],
      "metadata": {
        "id": "skBfK4Qj2P7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, tokenizer, bert_tc):\n",
        "    \"\"\"\n",
        "    BERTで固有表現抽出を行うための関数。\n",
        "    \"\"\"\n",
        "    # 符号化\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(\n",
        "        text, return_tensors='pt'\n",
        "    )\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
        "\n",
        "    # ラベルの予測値の計算\n",
        "    with torch.no_grad():\n",
        "        output = bert_tc(**encoding)\n",
        "        scores = output.logits\n",
        "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n",
        "\n",
        "    # ラベル列を固有表現に変換\n",
        "    entities = tokenizer.convert_bert_output_to_entities(\n",
        "        text, labels_predicted, spans\n",
        "    )\n",
        "\n",
        "    return entities\n",
        "\n",
        "# トークナイザのロード\n",
        "tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# ファインチューニングしたモデルをロードし、GPUにのせる。\n",
        "model = BertForTokenClassification_pl.load_from_checkpoint(\n",
        "    best_model_path\n",
        ")\n",
        "bert_tc = model.bert_tc.cuda()\n",
        "\n",
        "# 固有表現抽出\n",
        "# 注：以下ではコードのわかりやすさのために、1データづつ処理しているが、\n",
        "# バッチ化して処理を行った方が処理時間は短い\n",
        "entities_list = [] # 正解の固有表現を追加していく。\n",
        "entities_predicted_list = [] # 抽出された固有表現を追加していく。\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    entities_predicted = predict(text, tokenizer, bert_tc) # BERTで予測\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )"
      ],
      "metadata": {
        "id": "_DWck_Yd2X3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"# 正解\")\n",
        "print(entities_list[0])\n",
        "print(\"# 抽出\")\n",
        "print(entities_predicted_list[0])"
      ],
      "metadata": {
        "id": "LtQNR0I02xtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
        "    \"\"\"\n",
        "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
        "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
        "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
        "    \"\"\"\n",
        "    num_entities = 0 # 固有表現(正解)の個数\n",
        "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
        "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
        "\n",
        "    # それぞれの文章で予測と正解を比較。\n",
        "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
        "    for entities, entities_predicted \\\n",
        "        in zip(entities_list, entities_predicted_list):\n",
        "\n",
        "        if type_id:\n",
        "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
        "            entities_predicted = [ \n",
        "                e for e in entities_predicted if e['type_id'] == type_id\n",
        "            ]\n",
        "            \n",
        "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
        "        set_entities = set( get_span_type(e) for e in entities )\n",
        "        set_entities_predicted = \\\n",
        "            set( get_span_type(e) for e in entities_predicted )\n",
        "\n",
        "        num_entities += len(entities)\n",
        "        num_predictions += len(entities_predicted)\n",
        "        num_correct += len( set_entities & set_entities_predicted )\n",
        "\n",
        "    # 指標を計算\n",
        "    precision = num_correct/num_predictions # 適合率\n",
        "    recall = num_correct/num_entities # 再現率\n",
        "    f_value = 2*precision*recall/(precision+recall) # F値\n",
        "\n",
        "    result = {\n",
        "        'num_entities': num_entities,\n",
        "        'num_predictions': num_predictions,\n",
        "        'num_correct': num_correct,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f_value': f_value\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "IyArc_Iq25Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( evaluate_model(entities_list, entities_predicted_list) )"
      ],
      "metadata": {
        "id": "iqiIKZBU3DbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 固有表現抽出の実装：BIO法"
      ],
      "metadata": {
        "id": "cTtj8qkq3EQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NER_tokenizer_BIO(BertJapaneseTokenizer):\n",
        "\n",
        "    # 初期化時に固有表現のカテゴリーの数`num_entity_type`を\n",
        "    # 受け入れるようにする。\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.num_entity_type = kwargs.pop('num_entity_type')\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def encode_plus_tagged(self, text, entities, max_length):\n",
        "        \"\"\"\n",
        "        文章とそれに含まれる固有表現が与えられた時に、\n",
        "        符号化とラベル列の作成を行う。\n",
        "        \"\"\"\n",
        "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
        "        splitted = [] # 分割後の文字列を追加していく\n",
        "        position = 0\n",
        "        for entity in entities:\n",
        "            start = entity['span'][0]\n",
        "            end = entity['span'][1]\n",
        "            label = entity['type_id']\n",
        "            splitted.append({'text':text[position:start], 'label':0})\n",
        "            splitted.append({'text':text[start:end], 'label':label})\n",
        "            position = end\n",
        "        splitted.append({'text': text[position:], 'label':0})\n",
        "        splitted = [ s for s in splitted if s['text'] ]\n",
        "\n",
        "        # 分割されたそれぞれの文章をトークン化し、ラベルをつける。\n",
        "        tokens = [] # トークンを追加していく\n",
        "        labels = [] # ラベルを追加していく\n",
        "        for s in splitted:\n",
        "            tokens_splitted = self.tokenize(s['text'])\n",
        "            label = s['label']\n",
        "            if label > 0: # 固有表現\n",
        "                # まずトークン全てにI-タグを付与\n",
        "                labels_splitted =  \\\n",
        "                    [ label + self.num_entity_type ] * len(tokens_splitted)\n",
        "                # 先頭のトークンをB-タグにする\n",
        "                labels_splitted[0] = label\n",
        "            else: # それ以外\n",
        "                labels_splitted =  [0] * len(tokens_splitted)\n",
        "            \n",
        "            tokens.extend(tokens_splitted)\n",
        "            labels.extend(labels_splitted)\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens)\n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids, \n",
        "            max_length=max_length, \n",
        "            padding='max_length',\n",
        "            truncation=True\n",
        "        ) \n",
        "\n",
        "        # ラベルに特殊トークンを追加\n",
        "        labels = [0] + labels[:max_length-2] + [0]\n",
        "        labels = labels + [0]*( max_length - len(labels) )\n",
        "        encoding['labels'] = labels\n",
        "\n",
        "        return encoding\n",
        "\n",
        "    def encode_plus_untagged(\n",
        "        self, text, max_length=None, return_tensors=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
        "        IO法のトークナイザのencode_plus_untaggedと同じ\n",
        "        \"\"\"\n",
        "        # 文章のトークン化を行い、\n",
        "        # それぞれのトークンと文章中の文字列を対応づける。\n",
        "        tokens = [] # トークンを追加していく。\n",
        "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
        "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
        "        for word in words:\n",
        "            # 単語をサブワードに分割\n",
        "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
        "            tokens.extend(tokens_word)\n",
        "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
        "                tokens_original.append(word)\n",
        "            else:\n",
        "                tokens_original.extend([\n",
        "                    token.replace('##','') for token in tokens_word\n",
        "                ])\n",
        "\n",
        "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
        "        position = 0\n",
        "        spans = [] # トークンの位置を追加していく。\n",
        "        for token in tokens_original:\n",
        "            l = len(token)\n",
        "            while 1:\n",
        "                if token != text[position:position+l]:\n",
        "                    position += 1\n",
        "                else:\n",
        "                    spans.append([position, position+l])\n",
        "                    position += l\n",
        "                    break\n",
        "\n",
        "        # 符号化を行いBERTに入力できる形式にする。\n",
        "        input_ids = self.convert_tokens_to_ids(tokens) \n",
        "        encoding = self.prepare_for_model(\n",
        "            input_ids, \n",
        "            max_length=max_length, \n",
        "            padding='max_length' if max_length else False, \n",
        "            truncation=True if max_length else False\n",
        "        )\n",
        "        sequence_length = len(encoding['input_ids'])\n",
        "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
        "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
        "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
        "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
        "\n",
        "        # 必要に応じてtorch.Tensorにする。\n",
        "        if return_tensors == 'pt':\n",
        "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
        "\n",
        "        return encoding, spans\n",
        "\n",
        "    @staticmethod\n",
        "    def Viterbi(scores_bert, num_entity_type, penalty=10000):\n",
        "        \"\"\"\n",
        "        Viterbiアルゴリズムで最適解を求める。\n",
        "        \"\"\"\n",
        "        m = 2*num_entity_type + 1\n",
        "        penalty_matrix = np.zeros([m, m])\n",
        "        for i in range(m):\n",
        "            for j in range(1+num_entity_type, m):\n",
        "                if not ( (i == j) or (i+num_entity_type == j) ): \n",
        "                    penalty_matrix[i,j] = penalty\n",
        "        \n",
        "        path = [ [i] for i in range(m) ]\n",
        "        scores_path = scores_bert[0] - penalty_matrix[0,:]\n",
        "        scores_bert = scores_bert[1:]\n",
        "\n",
        "        for scores in scores_bert:\n",
        "            assert len(scores) == 2*num_entity_type + 1\n",
        "            score_matrix = np.array(scores_path).reshape(-1,1) \\\n",
        "                + np.array(scores).reshape(1,-1) \\\n",
        "                - penalty_matrix\n",
        "            scores_path = score_matrix.max(axis=0)\n",
        "            argmax = score_matrix.argmax(axis=0)\n",
        "            path_new = []\n",
        "            for i, idx in enumerate(argmax):\n",
        "                path_new.append( path[idx] + [i] )\n",
        "            path = path_new\n",
        "\n",
        "        labels_optimal = path[np.argmax(scores_path)]\n",
        "        return labels_optimal\n",
        "\n",
        "    def convert_bert_output_to_entities(self, text, scores, spans):\n",
        "        \"\"\"\n",
        "        文章、分類スコア、各トークンの位置から固有表現を得る。\n",
        "        分類スコアはサイズが（系列長、ラベル数）の2次元配列\n",
        "        \"\"\"\n",
        "        assert len(spans) == len(scores)\n",
        "        num_entity_type = self.num_entity_type\n",
        "        \n",
        "        # 特殊トークンに対応する部分を取り除く\n",
        "        scores = [score for score, span in zip(scores, spans) if span[0]!=-1]\n",
        "        spans = [span for span in spans if span[0]!=-1]\n",
        "\n",
        "        # Viterbiアルゴリズムでラベルの予測値を決める。\n",
        "        labels = self.Viterbi(scores, num_entity_type)\n",
        "\n",
        "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
        "        entities = []\n",
        "        for label, group \\\n",
        "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "            \n",
        "            group = list(group)\n",
        "            start = spans[group[0][0]][0]\n",
        "            end = spans[group[-1][0]][1]\n",
        "\n",
        "            if label != 0: # 固有表現であれば\n",
        "                if 1 <= label <= num_entity_type:\n",
        "                     # ラベルが`B-`ならば、新しいentityを追加\n",
        "                    entity = {\n",
        "                        \"name\": text[start:end],\n",
        "                        \"span\": [start, end],\n",
        "                        \"type_id\": label\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "                else:\n",
        "                    # ラベルが`I-`ならば、直近のentityを更新\n",
        "                    entity['span'][1] = end \n",
        "                    entity['name'] = text[entity['span'][0]:entity['span'][1]]\n",
        "                \n",
        "        return entities"
      ],
      "metadata": {
        "id": "azVo_tyI3OVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークナイザのロード\n",
        "# 固有表現のカテゴリーの数`num_entity_type`を入力に入れる必要がある。\n",
        "tokenizer = NER_tokenizer_BIO.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_entity_type=8 \n",
        ")\n",
        "\n",
        "# データセットの作成\n",
        "max_length = 128\n",
        "dataset_train_for_loader = create_dataset(\n",
        "    tokenizer, dataset_train, max_length\n",
        ")\n",
        "dataset_val_for_loader = create_dataset(\n",
        "    tokenizer, dataset_val, max_length\n",
        ")\n",
        "\n",
        "# データローダの作成\n",
        "dataloader_train = DataLoader(\n",
        "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
        ")\n",
        "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
      ],
      "metadata": {
        "id": "1WLb-bA_3o5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファインチューニング\n",
        "checkpoint = pl.callbacks.ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top_k=1,\n",
        "    save_weights_only=True,\n",
        "    dirpath='model_BIO/'\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    max_epochs=5,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# PyTorch Lightningのモデルのロード\n",
        "num_entity_type = 8\n",
        "num_labels = 2*num_entity_type+1\n",
        "model = BertForTokenClassification_pl(\n",
        "    MODEL_NAME, num_labels=num_labels, lr=1e-5\n",
        ")\n",
        "\n",
        "# ファインチューニング\n",
        "trainer.fit(model, dataloader_train, dataloader_val)\n",
        "best_model_path = checkpoint.best_model_path\n",
        "\n",
        "# 性能評価\n",
        "model = BertForTokenClassification_pl.load_from_checkpoint(\n",
        "    best_model_path\n",
        ") \n",
        "bert_tc = model.bert_tc.cuda()\n",
        "\n",
        "entities_list = [] # 正解の固有表現を追加していく\n",
        "entities_predicted_list = [] # 抽出された固有表現を追加していく\n",
        "for sample in tqdm(dataset_test):\n",
        "    text = sample['text']\n",
        "    encoding, spans = tokenizer.encode_plus_untagged(\n",
        "        text, return_tensors='pt'\n",
        "    )\n",
        "    encoding = { k: v.cuda() for k, v in encoding.items() } \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = bert_tc(**encoding)\n",
        "        scores = output.logits\n",
        "        scores = scores[0].cpu().numpy().tolist()\n",
        "        \n",
        "    # 分類スコアを固有表現に変換する\n",
        "    entities_predicted = tokenizer.convert_bert_output_to_entities(\n",
        "        text, scores, spans\n",
        "    )\n",
        "\n",
        "    entities_list.append(sample['entities'])\n",
        "    entities_predicted_list.append( entities_predicted )\n",
        "\n",
        "print(evaluate_model(entities_list, entities_predicted_list))"
      ],
      "metadata": {
        "id": "WNrgNamx3vHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}